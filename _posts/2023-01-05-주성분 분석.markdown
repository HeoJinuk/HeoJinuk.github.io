```python
import numpy as np
import matplotlib.pyplot as plt
```

# 주성분 분석

데이터 분석에서는 가능한 한 많은 정보를 유지하면서 고차원의 데이터를 저차원의 데이터로 변환하는 **차원 축소**(dimensionally reduction)을 하는 경우가 있다.
- 차원 축소에 사용하는 대표적인 방법으로 **주성분 분석**(principal component analysis)이 있다.

## 주성분 분석

어느 학교 학생들이 국어 시험과 영어 시험을 봤다고 해보자.

그 결과 중 일부가 다음과 같았다고 하자.

|학생|국어 점수|영어 점수|
|:---:|:---:|:---:|
|A|92|81|
|B|92|83|
|C|94|81|
|D|94|85|
|$\vdots$|$\vdots$|$\vdots$|

국어 성적을 x축에, 영어 성적을 y축에 놓고 데이터의 일부를 시각화하면 다음과 같다.


```python
korean  = [92, 92, 94, 94]
english = [81, 83, 81, 85]

plt.scatter(korean, english, zorder=10)

arrow_style={'width':0.005, 'head_width':0.1, 'length_includes_head':False, 'zorder':10}
plt.arrow(89, 80, 6, 0, **arrow_style)
plt.arrow(90, 79, 0, 6, **arrow_style)

plt.xlim(89.5, 95.2)
plt.ylim(79.5, 85.2)

plt.xlabel('Korean')
plt.ylabel('English')

plt.grid(True)
plt.show()
```


    
![png](/assets/img/posts/PCA/output_7_0.png)
    


4 학생의 국어 성적의 평균은 93, 영어 성적의 평균은 82.5이다.
- 평균들을 하나의 벡터로 나타내면, $\begin{bmatrix}93 \\ 82.5\end{bmatrix}$이다.
- 이 벡터는 각 데이터의 평균으로 이루져 있으므로, 평균벡터라고 한다.
- $n$차원의 데이터 $x = \{x_1, x_2, \cdots, x_k\}$에 대해 **평균벡터**(mean vector) $m$은 다음과 같이 정의된다.


$$m = \frac{1}{k} \sum_{i=1}^k x_i$$


    - $m$은 $n$차원 벡터이다.

> (참고) 데이터 $x$는 개수가 $k$개, 변량이 $n$개인 $n \times k$ 행렬이다.

주성분 분석에서는 데이터의 중심을 원점으로 하기 때문에, 데이터의 중심인 평균벡터가 영벡터가 되도록 데이터를 변환한다.
- 다음과 같이 각 데이터 $x_i$에서 평균벡터 $m$을 빼면, 변환된 데이터의 평균벡터는 영벡터가 된다.

$$x_i \leftarrow x_i - m$$


$$
\begin{align*}
\begin{matrix}
\text{변환된 데이터}\ :
&x_1 = \left[\begin{matrix}92 \\ 81\end{matrix}\right] - \left[\begin{matrix}93 \\ 82.5\end{matrix}\right] = \left[\begin{matrix}-1 \\ -1.5\end{matrix}\right] 
&x_2 = \left[\begin{matrix}92 \\ 83\end{matrix}\right] - \left[\begin{matrix}93 \\ 82.5\end{matrix}\right] = \left[\begin{matrix}-1 \\ 0.5\end{matrix}\right]\\
&x_3 = \left[\begin{matrix}94 \\ 81\end{matrix}\right] - \left[\begin{matrix}93 \\ 82.5\end{matrix}\right] = \left[\begin{matrix}1 \\ -1.5\end{matrix}\right] 
&x_4 = \left[\begin{matrix}94 \\ 85\end{matrix}\right] - \left[\begin{matrix}93 \\ 82.5\end{matrix}\right] = \left[\begin{matrix}1 \\ 2.5\end{matrix}\right] \\
\end{matrix}
\end{align*}
$$


<br>
$$
\text{변환된 데이터의 평균벡터}\ :\ \frac{1}{4}\left(
\left[\begin{matrix}-1 \\ -1.5 \end{matrix}\right]  
+ \left[\begin{matrix}-1 \\ 0.5 \end{matrix}\right]
+ \left[\begin{matrix}1 \\ -1.5 \end{matrix}\right]
+ \left[\begin{matrix}1 \\ 2.5\end{matrix}\right]
\right)
= \left[\begin{matrix}0 \\ 0 \end{matrix}\right]  
$$

변환된 데이터를 그려보면 다음과 같이 된다.

![png](/assets/img/posts/PCA/fig1.png)

이제 변환된 데이터를 가능한 한 많은 정보를 유지하면서 고차원의 데이터를 저차원의 데이터로 변환하려고 한다.  
다음 중 어떤 경우가 가능한 많은 정보를 유지하는 경우일까?

위의 예시에 해당하는 국어, 영어 성적의 2차원 데이터를 1차원으로 차원 축소하는 경우를 살펴보자.
- 국어, 영어 성적의 2차원 데이터를 어떤 단위 벡터 $u$로 정사영함으로써 1차원으로 차원 축소를 한다.

![png](/assets/img/posts/PCA/fig2.png)

- (a)는 4개의 서로 다른 데이터를 $u = \left[\begin{matrix}1 \\ 0 \end{matrix}\right]$에 정사영하여 2개의 위치로 변환된 경우이고, 
- (b)는 4개의 서로 다른 데이터를 $u = \left[\begin{matrix}0 \\ 1 \end{matrix}\right]$에 정사영하여 3개의 위치로 변환된 경우이고, 
- (c)는 4개의 서로 다른 데이터를 $u = \left[\begin{matrix}\frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}} \end{matrix}\right]$에 정사영하여 4개의 위치로 변환된 경우이다.

이 중 가능한 한 정보를 유지하면서 고차원의 데이터를 저차원의 데이터로 변환된 경우는 (c)이다. 
- (a), (b), (c) 모두 2차원의 서로 다른 4개의 데이터를 각각의 벡터로 정사영하여 1차원 데이터로 차원 축소하였다.
- 차원 축소를 하면 구분 가능한 데이터의 개수가 (a)는 4개에서 2개로, (b)는 4개에서 3개로 감소하게 된다.
- 그러나 (c)는 구분 가능한 데이터의 개수가 4개로, 기존의 4개의 데이터를 모두 구분할 수 있다.
- 그러므로, (a)와 (b)에 비해 (c)가 가장 많은 정보를 유지하면서 2차원의 데이터를 1차원으로 축소한 경우이다.

위의 예제에서 알 수 있듯이, 가능한 한 많은 정보를 유지하면서 고차원의 데이터를 저차원의 데이터로 변환하려면, 차원 축소하였을 때의 데이터가 최대한 많이 구분되도록 하는 $u$를 찾는 것이 중요하다.
- 그렇다면 무수히 많은 단위 벡터 $u$ 중 데이터가 최대한 많이 구분되도록 하는 가장 좋은 벡터는 어떻게 찾을 수 있을까?
- 데이터를 최대한 많이 구분하려면, 데이터가 퍼져 있을수록 구분하기 쉽다.
    - 그러므로, 데이터의 퍼진 정도를 나타내는 **분산**을 척도로, 차원 축소를 하였을 때 분산을 가장 크게 하는 벡터 $u$를 찾으면 된다.

이제 이 내용을 수학적으로 표현해보자.
- 평균벡터가 영벡터인 $n$차원 데이터 $x = \{x_1, x_2, \cdots, x_n\}$을 새로운 기저 $u = \{u_1, u_2, \cdots, u_n\}$ 의 좌표계로 선형변환한다고 하자.
    - 단, $u_i (i=1, 2, \cdots, n)$들은 서로 직교하는 단위벡터이다.
    - 즉, $|u_i| = 1$이다.
- 이때 임의의 $n$차원 데이터 $x$는 다음과 같이 기저벡터들의 선형결합으로 나타낼 수 있다.

$$
\begin{align*}
x 
&= \sum_{i=1}^n \left(x^\top u_i\right)u_i \\
&= \left(x^\top u_1\right)u_1 + \left(x^\top u_2\right)u_2 + \cdots + \left(x^\top u_n\right)u_n
\end{align*}
$$

- 여기서 $x^\top u_i = x \cdot u_i$는 $x$를 $u_i$ 방향으로 정사영한 벡터의 크기에 해당한다.

$u_1 = \left[\begin{matrix}\frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}} \end{matrix}\right]$, $u_2 = \left[\begin{matrix}-\frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}} \end{matrix}\right]$라고 하면 아래와 같이 점 $x_4 = \left[\begin{matrix}1\\ 2.5 \end{matrix}\right]$를 선형결합으로 나타낼 수 있다.


```python
arrow_style = {'width':0.005, 'head_width':0.1, 'length_includes_head':True, 'zorder':10}

student4 = [1, 2.5]

plt.figure(figsize=(7,7))
plt.scatter(*student4, zorder=10)
plt.arrow(0, 0, *student4, length_includes_head=True, head_width=0.1, color='red')

plt.arrow(-3, 0, 6, 0, **arrow_style, color='gray')
plt.arrow(0, -3, 0, 6, **arrow_style, color='gray')
plt.arrow(0, 0, 1, 0, **arrow_style, color='black')
plt.arrow(0, 0, 0, 1, **arrow_style, color='black')

L = 1/(2**0.5)
plt.arrow(-3+L, -3+L,  6-2*L, 6-2*L, **arrow_style, color='lightgreen')
plt.arrow( 3-L, -3+L, -6+2*L, 6-2*L, **arrow_style, color='lightgreen')
plt.arrow(0, 0,  L, L, **arrow_style, color='green')
plt.arrow(0, 0, -L, L, **arrow_style, color='green')

dot1 = np.dot(np.dot(student4, [ L, L]), [ L, L])
dot2 = np.dot(np.dot(student4, [-L, L]), [-L, L])

plt.arrow( 0, 0, *dot1, length_includes_head=True, head_width=0.1, color='red', zorder=10, linestyle='-.')
plt.arrow( 0, 0, *dot2, length_includes_head=True, head_width=0.1, color='red', zorder=10, linestyle='-.')
plt.arrow(*dot1, *dot2, length_includes_head=True, color='red', zorder=10, linestyle=':')
plt.arrow(*dot2, *dot1, length_includes_head=True, color='red', zorder=10, linestyle=':')

plt.annotate('$e_1$', xy=( 0.8, -0.2))
plt.annotate('$e_2$', xy=(-0.25,  0.8))
plt.annotate('$u_1$', xy=( L-0.05, L-0.2))
plt.annotate('$u_2$', xy=(-L-0.15, L-0.2))
plt.annotate('$x = 2.47 u_1 + 1.06 u_2$'  , xy=(1, 2.6))

plt.xlabel('Korean')
plt.ylabel('English')

plt.grid(True)
plt.show()
```


    
![png](/assets/img/posts/PCA/output_22_0.png)
    


주성분 분석을 수행하기 위해서는 정사영한 데이터의 분산을 가장 크게 하는 기저벡터 $u$를 찾아야 하므로, 정사영한 데이터의 분산을 계산해보자.

$$Var(x^\top u) = \frac{1}{n}\sum_{i=1}^{n}\left(x^\top u_i - E(x^\top u_i)\right)^2$$

이 때, $x$의 평균벡터는 영벡터이므로,

$$Var(x^\top u) = \frac{1}{n}\sum_{i=1}^{n}\left(x^\top u_i - 0\right)^2 =\frac{1}{n}\sum_{i=1}^{n}\left(x^\top u_i\right)^2$$

이다.

그러므로

$$
\begin{align*}
Var(x^\top u) 
&= \frac{1}{n}\left(x^\top u\right)^\top\left(x^\top u\right) \\
&= \frac{1}{n}u^\top x x^\top u \\
&= \frac{1}{n}u^\top(x x^\top)u \\
&= u^\top\left(\frac{x x^\top}{n}\right)u
\end{align*}
$$

으로 분산을 구할 수 있다.

이제 위 식을 최대가 되도록 하는 기저 벡터 $u$를 찾으면 된다.

그런데 식을 다시 보면, $\frac{x x^\top}{n}$가 포함되어 있다.
- 이는 데이터의 구조를 설명해주는 공분산 행렬이다.

공분산 행렬은 일종의 행렬로써, 데이터의 구조를 설명해주며, 특히 특징 쌍(feature pairs)들의 변동이 얼마나 닮았는가(다시 말하면, 한 feature가 변할 때, 다른 feature가 얼마만큼 함께 변하는가)를 행렬에 나타내고 있다.
- $n$차원의 데이터 $\{x_1, x_2, \cdots, x_k\}$에 대해 **공분산 행렬**(covariance matrix) $C$는 다음과 같이 정의된다.
- 여기서 $C$는 $n \times n$ 행렬이다.

$$C = \frac{1}{k} \sum_{i=1}^k (x_i - m)(x_i - m)^\top$$

위의 4개의 데이터에 대해 공분산을 계산해보자.
- 데이터는 2차원 데이터이므로, 이 데이터의 공분산 행렬 $C$는 $2 \times 2$ 행렬이다.

$$C = \left[\begin{matrix}xx & xy \\ yx & yy\end{matrix}\right]$$

각각의 원소는 다음과 같은 의미를 갖는다.
- 1행 1열의 원소 xx는 x의 분산(${S_x}^2$)으로, x축 방향으로 퍼질 때(변할 때) x축 방향으로 퍼지는(변하는) 정도
- 1행 2열의 원소 xy는 x와 y의 공분산(${S_{xy}}$)으로, x축 방향으로 퍼질 때(변할 때) y축 방향으로 퍼지는(변하는) 정도
- 2행 1열의 원소 yx는 y와 x의 공분산(${S_{yx}}$)으로, y축 방향으로 퍼질 때(변할 때) x축 방향으로 퍼지는(변하는) 정도
- 2행 2열의 원소 yy은 y의 분산(${S_y}^2$)으로, y축 방향으로 퍼질 때(변할 때) y축 방향으로 퍼지는(변하는) 정도  

예를 들어, 공분산 행렬이 $C = \left[\begin{matrix}3 & 2 \\ 2 & 4 \end{matrix}\right]$ 일 때, 어떤 벡터 $a = \left[\begin{matrix}x \\ y\end{matrix}\right]$에 공분산 행렬 $C$를 곱하면

$$C a = \left[\begin{matrix}3 & 2 \\ 2 & 4 \end{matrix}\right] \left[\begin{matrix}x \\ y\end{matrix}\right] = \left[\begin{matrix}3x + 2y \\ 2x + 4y\end{matrix}\right]$$

이다.

즉, 벡터 $a$의 기저가 $\vec{v_1}, \vec{v_2}$라고 하면,

- 1행 1열의 원소 3에 의해, 원래 기저에서 $\vec{v_1}$ 방향으로 1배 변할 때, 표준 기저에서는 $\vec{e_1}$ 방향으로 3배 변한다.
- 1행 2열의 원소 2에 의해, 원래 기저에서 $\vec{v_1}$ 방향으로 1배 변할 때, 표준 기저에서는 $\vec{e_2}$ 방향으로 2배 변한다.
- 2행 1열의 원소 2에 의해,  원래 기저에서 $\vec{v_2}$ 방향으로 1배 변할 때, 표준 기저에서는 $\vec{e_1}$ 방향으로 2배 변한다.
- 2행 2열의 원소 4에 의해, 원래 기저에서 $\vec{v_2}$ 방향으로 1배 변할 때, 표준 기저에서는 $\vec{e_2}$ 방향으로 4배 변한다.

를 의미한다.

즉, 벡터 $a$를 공분산 행렬 $C$로 선형변환하면, $x$는 x축 방향으로 3배만큼, y축 방향으로 2배만큼 퍼지게 되고, $y$는 x축 방향으로 2배만큼, y축 방향으로 4배만큼 퍼지게 된다.
- 이를 그림으로 나타내면 아래와 같이 된다.


```python
mean = [0,0]
cov = [[1,0], [0,1]]
data = np.random.multivariate_normal(mean, cov, 1000)

C = np.array([[3,2], [2,4]])
# C = np.array([[2,1], [1,2]])

new_data = data@C

fig, axs = plt.subplots(1,2, figsize=(12,5))

axs[0].scatter(data[:, 0], data[:, 1], alpha=0.5)
axs[0].set_title('Before')
axs[0].grid(True)
axs[0].set_xlim(-10,10)
axs[0].set_ylim(-10,10)


axs[1].scatter(new_data[:, 0], new_data[:, 1], alpha=0.5)
axs[1].set_title('After')
axs[1].grid(True)
axs[1].set_xlim(-10,10)
axs[1].set_ylim(-10,10)

a, b = np.linalg.eig(C)
axs[0].arrow(0, 0, 1, 0, color='black', **arrow_style)
axs[0].arrow(0, 0, 0, 1, color='black', **arrow_style)
axs[1].arrow(0, 0, *b[:, 0], color='black', **arrow_style)
axs[1].arrow(0, 0, *b[:, 1], color='black', **arrow_style)

points = np.array([[0,0], [1,1], [-1,-1], [1,-1], [-1,1]])
new_points = points@C

for p, n in zip(points, new_points):
    ps = a*(p@b)
    axs[0].annotate(f'{p}', p)
    axs[1].annotate(f'{p}→{n}', n)

axs[0].scatter(points[:, 0], points[:, 1], c='r')
axs[1].scatter(new_points[:, 0], new_points[:, 1], c='r')

plt.show()
```


    
![png](/assets/img/posts/PCA/output_32_0.png)
    


원래 원형으로 모여있던 점들이 공분산 행렬을 곱함으로써, 어떤 방향으로 퍼지는 것을 확인할 수 있다.
- $x$는 x축 방향으로 3배만큼, y축 방향으로 2배만큼 퍼지게 되고, $y$는 x축 방향으로 2배만큼, y축 방향으로 4배만큼 퍼지게 된다.
- (1, 1)이 (5, 6)으로, (-1, 1)이 (-1, 2)로, (-1, -1)이 (-5, -6)으로,  (1, -1)이 (1, -2)로 퍼진다. 

다시, 정사영한 데이터의 분산을 공분산 행렬 $C$로 나타내면,

$$
\begin{align*}
Var(x^\top u) 
&= u^\top\left(\frac{x x^\top}{n}\right)u \\
&= u^\top C u
\end{align*}
$$

이고, 이 값을 최대화하는 벡터 $u$를 찾으면 된다.
- 단, 벡터 $u$는 단위벡터로, 그 크기는 1이다.

이 문제는 라그랑주 승수법(Lagrange multiplier method)를 사용해 풀 수 있다.
- 라그랑주 승수 $\lambda$를 사용하여 최대화할 식을 바꾸면,
$$L = u^\top C u - \lambda ( u^\top u - 1)$$
    이고, 이 식의 해는 $L$을 $u$에 대한 편미분했을 때 영벡터가 되도록 하는 $u$이다.
- 즉, 
$$\frac{\partial L}{\partial u} = 2Cu-2\lambda u = 0$$
    이므로,
$$Cu = \lambda u$$
    를 만족하는 $u$를 구하면 된다. 

이것은 $C$에 대한 고유벡터와 고유값을 구하는 문제로 바꿀 수 있다.
- $u$가 $C$의 고유벡터이고, $\lambda$가 $C$의 고유값이다.

![png](/assets/img/posts/PCA/fig3.png)

또한, $Cu = \lambda u$ 의 양변 앞에 $u$를 전치한 $u^\top$을 곱하면, 고유값 $\lambda$가 분산이 된다.

$$Var(x^\top u) = u^\top C u = u^\top \lambda u = \lambda u^\top u = \lambda$$

위에서 가능한 한 많은 정보를 유지하기 위해서는 데이터가 퍼져 있어야 한다고 했다.
- 그리고 데이터의 분산이 클수록, 크게 퍼져 있게 된다.
- 따라서 분산인 고유값이 큰 순서대로 고유벡터를 정렬하면, 벡터 $u$가 데이터를 크게 퍼트리는 순으로 정렬된다.
- 즉, 고유값이 가장 클 때의 고유벡터를 찾으면, 우리가 원하던 분산을 가장 크게 하는 기저 벡터 $u$를 찾을 수 있다.

정리하면, 주성분 분석에서는 가능한 한 많은 정보를 유지하면서 고차원의 데이터를 저차원의 데이터로 변환하는 차원 축소를 하기 위해, 어떤 기저 벡터로 정사영했을 때 분산이 가장 크게 되는 기저 벡터 $u$를 찾는다.
- 정사영했을 때의 분산을 계산해보면, 분산은 $u^\top \frac{x x^\top}{n} u = u^\top C u$이다.
- 이때, $C$는 데이터의 공분산 행렬로, 공분산 행렬을 통해 데이터가 어떤 방향으로 퍼지는 지를 알 수 있다.
- 이 공분산 행렬의 고유벡터가 데이터가 퍼지는 방향이고, 고유값이 데이터가 퍼지는 정도 (분산)이므로 고유값이 클수록 데이터가 더 크게 퍼지게 된다.
- 그러므로, 고유값이 가장 클 때의 고유벡터를 찾으면, 그 고유벡터가 정사영했을 때 분산이 가장 크게 되는 기저 벡터 $u$이다.
- 이 기저벡터 $u$를 제1주성분이라고 한다.

위의 국어, 영어 성적 예시를 풀어보면,


```python
korean_norm = korean - np.mean(korean)
english_norm = english - np.mean(english)
korean_norm, english_norm
```




    (array([-1., -1.,  1.,  1.]), array([-1.5,  0.5, -1.5,  2.5]))




```python
C = np.cov(np.array([korean_norm, english_norm]), ddof=0)
C
```




    array([[1.  , 0.5 ],
           [0.5 , 2.75]])




```python
eig_val, eig_vec = np.linalg.eig(C)
eig_val, eig_vec
```




    (array([0.86721778, 2.88278222]),
     array([[-0.96649965, -0.25666794],
            [ 0.25666794, -0.96649965]]))




```python
index = np.argsort(-eig_val)
pca = eig_vec[:, index]
pca
```




    array([[-0.25666794, -0.96649965],
           [-0.96649965,  0.25666794]])



가장 분산을 크게 하는 벡터는 $\left[\begin{matrix}-0.257 \\ -0.966\end{matrix}\right]$이다.


```python
(np.array([korean_norm, english_norm]).T@pca)
```




    array([[ 1.70641741,  0.58149775],
           [-0.22658189,  1.09483362],
           [ 1.19308154, -1.35150155],
           [-2.67291706, -0.32482981]])




```python
plt.scatter(korean_norm, english_norm, zorder=10)

arrow_style={'width':0.005, 'head_width':0.1, 'length_includes_head':False, 'zorder':10}
plt.arrow(-3, 0, 6, 0, color='black', **arrow_style)
plt.arrow(0, -3, 0, 6, color='black', **arrow_style)

f1 = lambda x:  0.966/0.257 * x
f2 = lambda x: -0.257/0.966 * x

plt.arrow(0, 0, -0.257, -0.966, color='red', **arrow_style)
plt.arrow(0, 0, -0.966,  0.257, color='pink', **arrow_style)

plt.plot([-1, 1], [f1(-1), f1(1)], color='green')
plt.plot([-2, 2], [f2(-2), f2(2)], color='lightgreen')

plt.xlabel('Korean')
plt.ylabel('English')

plt.grid(True)
plt.show()
```


    
![png](/assets/img/posts/PCA/output_48_0.png)
    



```python
np.array([korean_norm, english_norm]).T
```




    array([[-1. , -1.5],
           [-1. ,  0.5],
           [ 1. , -1.5],
           [ 1. ,  2.5]])




```python
new_zw = (np.array([korean_norm, english_norm]).T)@pca
new_zw
```




    array([[ 1.70641741,  0.58149775],
           [-0.22658189,  1.09483362],
           [ 1.19308154, -1.35150155],
           [-2.67291706, -0.32482981]])




```python
np.cov(new_zw.T, ddof=0).round(3)
```




    array([[ 2.883, -0.   ],
           [-0.   ,  0.867]])



주성분 분석을 통해 국어, 영어 성적의 2차원 데이터를 1차원 데이터로 차원 축소하면 

$$
\begin{align*}
\begin{matrix}
\text{차원 축소된 데이터}\ :
&x_1 = \left[\begin{matrix}-1 \\ -1.5\end{matrix}\right] \rightarrow \left[\begin{matrix}1.71\end{matrix}\right] 
&x_2 = \left[\begin{matrix}-1 \\ 0.5\end{matrix}\right] \rightarrow \left[\begin{matrix}-0.23\end{matrix}\right]\\
&x_3 = \left[\begin{matrix}1 \\ -1.5\end{matrix}\right] \rightarrow \left[\begin{matrix}1.19\end{matrix}\right]
&x_4 = \left[\begin{matrix}1 \\ 2.5\end{matrix}\right] \rightarrow \left[\begin{matrix}-2.67\end{matrix}\right] \\
\end{matrix}
\end{align*}
$$

가 된다.

여기서는 가능한 한 많은 정보를 유지하면서 고차원의 데이터를 저차원의 데이터로 변환하는 주성분 분석에 대해 설명하였다.
- 가능한 한 많은 정보를 유지하기 위해서는 데이터가 퍼져 있어야 하므로, 분산이 큰 축으로 차원을 축소하였다.
- 즉, 데이터의 분산이 크면 클수록 많은 정보를 담고 있다고 할 수 있다.

이를 바탕으로, 차원 축소하였을 때 차원 축소된 데이터가 원래 데이터의 어느 정도의 정보를 담고 있는 지를 나타내는 **기여율**을 계산해보자.
- 정보의 크기는 분산과 비례하므로, 분산을 통해 계산한다.

기여율은 원래 데이터의 전체 분산 중 차원 축소한 데이터의 분산으로 계산한다.
- 차원 축소한 데이터의 분산은 각 $u_i$로 정사영한 데이터의 분산의 합으로 계산할 수 있다.
- 그런데 위에서 우리는 고유값이 각 $u_i$에서의 분산이라는 것을 알았다.
- 즉, 각 고유벡터의 고유값이 그 고유벡터로 정사영했을 때의 분산이다.

$$
\begin{align*}
\text{기여율} 
&= \frac{\text{차원 축소한 데이터의 분산}}{\text{원래 데이터의 전체 분산}}\\
&= \frac{Var(x^\top{u_1}) + Var(x^\top{u_2}) + \cdots + Var(x^\top{u_k})}{Var(x)}\\
&= \frac{\lambda_1 + \lambda_2 + \cdots + \lambda_k}{\lambda_1 + \lambda_2 + \cdots + \lambda_k + \cdots + \lambda_n}\\
\end{align*}
$$

위의 예시에서 기여율을 계산해본다.


```python
print('제1주성분의 기여율 =', eig_val[index[:1]].sum()/eig_val.sum())
```

    제1주성분의 기여율 = 0.768741924943285



```python
print('제2주성분의 기여율 =', eig_val[index[1:2]].sum()/eig_val.sum())
```

    제2주성분의 기여율 = 0.23125807505671503


제1주성분의 기여율은 0.769로, 원래 데이터의 정보의 76.9%를 담고 있다.

---

### 예제)

다음은 5과목에 대한 20명의 학생의 시험 성적이다.  
주성분 분석을 수행해보자.
- 단, 기여율이 80%가 넘도록 하자.

|출석번호|수학 x|과학 y|사회 u|영어 v|국어 w|
|:---:|:---:|:---:|:---:|:---:|:---:|
|1|71|64|83|100|71|
|2|34|48|67|57|68|
|3|58|59|78|87|66|
|4|41|51|70|60|72|
|5|69|56|74|81|66|
|6|64|65|82|100|71|
|7|16|45|63|7|59|
|8|59|59|78|59|62|
|9|57|54|84|73|72|
|10|46|54|71|43|62|
|11|23|49|64|33|70|
|12|39|48|71|29|66|
|13|46|55|68|42|61|
|14|52|56|82|67|60|
|15|39|53|78|52|72|
|16|23|43|63|35|59|
|17|37|45|67|39|70|
|18|52|51|74|65|69|
|19|63|56|79|91|70|
|20|39|49|73|64|60|


```python
data = [[71,  64,  83,  100,  71],
        [34,  48,  67,   57,  68],
        [58,  59,  78,   87,  66],
        [41,  51,  70,   60,  72],
        [69,  56,  74,   81,  66],
        [64,  65,  82,  100,  71],
        [16,  45,  63,    7,  59],
        [59,  59,  78,   59,  62],
        [57,  54,  84,   73,  72],
        [46,  54,  71,   43,  62],
        [23,  49,  64,   33,  70],
        [39,  48,  71,   29,  66],
        [46,  55,  68,   42,  61],
        [52,  56,  82,   67,  60],
        [39,  53,  78,   52,  72],
        [23,  43,  63,   35,  59],
        [37,  45,  67,   39,  70],
        [52,  51,  74,   65,  69],
        [63,  56,  79,   91,  70],
        [39,  49,  73,   64,  60]]

data = np.array(data)
data
```




    array([[ 71,  64,  83, 100,  71],
           [ 34,  48,  67,  57,  68],
           [ 58,  59,  78,  87,  66],
           [ 41,  51,  70,  60,  72],
           [ 69,  56,  74,  81,  66],
           [ 64,  65,  82, 100,  71],
           [ 16,  45,  63,   7,  59],
           [ 59,  59,  78,  59,  62],
           [ 57,  54,  84,  73,  72],
           [ 46,  54,  71,  43,  62],
           [ 23,  49,  64,  33,  70],
           [ 39,  48,  71,  29,  66],
           [ 46,  55,  68,  42,  61],
           [ 52,  56,  82,  67,  60],
           [ 39,  53,  78,  52,  72],
           [ 23,  43,  63,  35,  59],
           [ 37,  45,  67,  39,  70],
           [ 52,  51,  74,  65,  69],
           [ 63,  56,  79,  91,  70],
           [ 39,  49,  73,  64,  60]])




```python
# 평균벡터를 영벡터로 만든다
data_0 = data - data.mean(axis=0)
data_0.mean(axis=0).round(3)
```




    array([ 0.,  0., -0., -0.,  0.])




```python
# 공분산 행렬을 구한다
cov = np.cov(data_0.T, ddof=0)
cov.round(3)
```




    array([[231.24 ,  77.25 ,  84.82 , 325.92 ,  23.28 ],
           [ 77.25 ,  34.4  ,  31.65 , 115.35 ,   7.55 ],
           [ 84.82 ,  31.65 ,  44.348, 131.41 ,  11.215],
           [325.92 , 115.35 , 131.41 , 591.46 ,  50.89 ],
           [ 23.28 ,   7.55 ,  11.215,  50.89 ,  22.21 ]])




```python
# 공분산 행렬의 고유값, 고유벡터를 구한다
eigval, eigvec = np.linalg.eig(cov)
eigval, eigvec
```




    (array([844.4504101 ,  43.88464881,   6.56102841,  11.75323021,
             17.00818247]),
     array([[-0.491513  , -0.75598812,  0.14130133,  0.39445342,  0.10647703],
            [-0.1730531 , -0.23783551, -0.8895693 , -0.34925878, -0.01310165],
            [-0.19588597, -0.19864044,  0.41576229, -0.83516616,  0.22761783],
            [-0.82781535,  0.52573608,  0.0141702 ,  0.02333993, -0.19384988],
            [-0.06941201,  0.23676946, -0.12506525,  0.15613309,  0.9482106 ]]))




```python
# 고유값이 큰 순서로 고유벡터를 정렬한다.
index = np.argsort(-eigval)
pca = eigvec[:, index]
pca
```




    array([[-0.491513  , -0.75598812,  0.10647703,  0.39445342,  0.14130133],
           [-0.1730531 , -0.23783551, -0.01310165, -0.34925878, -0.8895693 ],
           [-0.19588597, -0.19864044,  0.22761783, -0.83516616,  0.41576229],
           [-0.82781535,  0.52573608, -0.19384988,  0.02333993,  0.0141702 ],
           [-0.06941201,  0.23676946,  0.9482106 ,  0.15613309, -0.12506525]])




```python
# 기여율을 계산한다.

total_var = eigval.sum()
current_var = [1]
for i, ind in enumerate(index, start=1):
    current_var.append(current_var[-1] - (eigval[ind]/total_var))
    print(f'제{i}주성분의 기여율 = {eigval[ind]/total_var:.3f}')
    
plt.plot(range(len(current_var)), current_var, marker='o')
plt.show()
```

    제1주성분의 기여율 = 0.914
    제2주성분의 기여율 = 0.048
    제3주성분의 기여율 = 0.018
    제4주성분의 기여율 = 0.013
    제5주성분의 기여율 = 0.007



    
![png](/assets/img/posts/PCA/output_68_1.png)
    


제1주성분만으로 기여율이 80%가 넘으므로, 5차원의 데이터를 1차원으로 축소한다.


```python
one_dim_data = data_0 @ pca[:, [0]]
one_dim_data 
```




    array([[-49.96661766],
           [  9.92668458],
           [-30.6235937 ],
           [  2.61818228],
           [-29.76064147],
           [-46.50319377],
           [ 62.09209738],
           [ -7.65862887],
           [-19.26918818],
           [ 14.21255304],
           [ 35.47667681],
           [ 30.0032295 ],
           [ 15.52438521],
           [-10.96612123],
           [  8.31053714],
           [ 35.81878274],
           [ 23.73315714],
           [ -7.50284536],
           [-36.34679483],
           [  0.88133926]])



기존의 5차원 데이터가 이렇게 1차원 데이터로 축소되었다.

여기서 조심해야 할 점은 이렇게 차원 축소한 벡터가 무엇을 의미하는 지 알 수 없다는 것이다.
- 이것은 분석자가 직접 해석해야 하는 부분이다.
- 이 문제에서는 차원 축소한 벡터가 의미하는 것은 종합 성적이다.
    - 단, 성적이 나쁠수록 값이 크다.

참고
- https://angeloyeo.github.io/2019/07/17/eigen_vector.html
- https://angeloyeo.github.io/2019/07/27/PCA.html
- 응용이 보이는 선형대수학 : 파이썬과 함께하는 선형대수학 이론과 응용
